{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identical-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets, transforms\n",
    "from datasets import Split_Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "test_dataset = datasets.ImageFolder('/gpfs/u/locker/200/CADS/datasets/ImageNet/val', transform=val_transforms)\n",
    "\n",
    "val_dataset = Split_Dataset('/gpfs/u/locker/200/CADS/datasets/ImageNet',  \\\n",
    "                    f'./calib_splits/am_imagenet_5percent_val.txt',\n",
    "                    transform=val_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=256, shuffle=True,\n",
    "            num_workers=20, pin_memory=True,\n",
    "        )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=256, shuffle=False,\n",
    "            num_workers=20, pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_3_models(list_ckpts):\n",
    "    model1 = models.resnet50().cuda()\n",
    "    model2 = models.resnet50().cuda()\n",
    "    model3 = models.resnet50().cuda()\n",
    "    sd = torch.load(f\"./dist_models/{list_ckpts[0]}/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "    ckpt = {k.replace(\"members.0.\",\"\"):v for k,v in sd['model'].items()}\n",
    "    model1.load_state_dict(ckpt)\n",
    "    model1.eval()\n",
    "\n",
    "    sd = torch.load(f\"./dist_models/{list_ckpts[1]}/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "    ckpt = {k.replace(\"members.0.\",\"\"):v for k,v in sd['model'].items()}\n",
    "    model2.load_state_dict(ckpt)\n",
    "    model2.eval()\n",
    "\n",
    "    sd = torch.load(f\"./dist_models/{list_ckpts[2]}/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "    ckpt = {k.replace(\"members.0.\",\"\"):v for k,v in sd['model'].items()}\n",
    "    model3.load_state_dict(ckpt)\n",
    "    model3.eval()\n",
    "\n",
    "    return model1, model2, model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "described-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class JSD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSD, self).__init__()\n",
    "        self.kl = torch.nn.KLDivLoss(reduction='sum', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.tensor, q: torch.tensor):\n",
    "        p = F.log_softmax(p, dim=-1)\n",
    "        q = F.log_softmax(q, dim=-1)\n",
    "        \n",
    "        p, q = p.view(-1, p.size(-1)), q.view(-1, q.size(-1))\n",
    "        m = (0.5 * (p + q)).log()\n",
    "        return 0.5 * (self.kl(m, p.log()) + self.kl(m, q.log()))\n",
    "\n",
    "class KLD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLD, self).__init__()\n",
    "        self.kl = torch.nn.KLDivLoss(reduction='sum', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.tensor, q: torch.tensor):\n",
    "        p = F.log_softmax(p, dim=-1)\n",
    "        q = F.log_softmax(q, dim=-1)\n",
    "        return self.kl(p,q)\n",
    "\n",
    "kl_div = KLD()\n",
    "js_div = JSD()\n",
    "\n",
    "class _ECELoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_bins=20):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, softmaxes, labels):\n",
    "#         softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        ece = torch.zeros(1, device=softmaxes.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece\n",
    "\n",
    "nll_criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "ece_criterion = _ECELoss().cuda()\n",
    "\n",
    "def compute_pair_consensus(pair_preds):\n",
    "    agree = (pair_preds[0] == pair_preds[1])\n",
    "    agree_correct = agree & (pair_preds[0] == target)\n",
    "    agree_wrong = agree & (pair_preds[0] != target)\n",
    "    disagree = (pair_preds[0] != pair_preds[1])\n",
    "    disagree_both_wrong = disagree & (pair_preds[0] != target) & (pair_preds[1] != target)\n",
    "    disagree_one_correct = disagree & (pair_preds[0] != target) & (pair_preds[1] == target) \n",
    "    disagree_one_correct2 = disagree & (pair_preds[1] != target) & (pair_preds[0] == target) \n",
    "    return agree.sum(), disagree.sum(), agree_correct.sum(), agree_wrong.sum(), disagree_both_wrong.sum(), disagree_one_correct.sum()+disagree_one_correct2.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marked-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts = ['ft_baseR_cos_lr0.003_bs256', 'ft_eqR_cos_lr0.003_bs256', 'ft_inv_cos_lr0.003_bs256']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UB: 0.8425399661064148 | ensem: 0.7884399890899658\n",
      "agree: 0.826686680316925 | disagree: 0.17331331968307495\n",
      "agree_correct: 0.7130333185195923 | agree_wrong: 0.11365331709384918\n",
      "disagree_1correct: 0.10659998655319214 | disagree_2wrong: 0.06671332567930222\n",
      "Ensemble Variance Logits: 0.8963876366615295\n",
      "Ensemble Variance: 0.00029170859488658607\n",
      "KL div: 0.34884077310562134\n"
     ]
    }
   ],
   "source": [
    "model1, model2, model3 = load_3_models(ckpts)\n",
    "\n",
    "w_acc = 0\n",
    "n_acc = 0\n",
    "ag_sum = 0\n",
    "ag_c_sum = 0\n",
    "ag_w_sum = 0\n",
    "dag_sum = 0\n",
    "dag_c_sum = 0\n",
    "dag_w_sum = 0\n",
    "avg_std_logits = 0.\n",
    "avg_std = 0.\n",
    "\n",
    "kld_sum = 0.\n",
    "# js_div = 0.\n",
    "\n",
    "ece_ensem, ece1, ece2, ece3 = 0., 0., 0., 0.\n",
    "nll_ensem, nll1, nll2, nll3 = 0., 0., 0., 0.\n",
    "\n",
    "pairs = ([0,1], [0,2], [1,2])\n",
    "targets = []\n",
    "for it, (img,target) in enumerate(test_loader):\n",
    "    target = target.cuda(non_blocking=True)\n",
    "    img = img.cuda(non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        output1 = model1(img)\n",
    "        output2 = model2(img)\n",
    "        output3 = model3(img)\n",
    "        preds = torch.stack([output1,output2,output3])\n",
    "        avg_std_logits += torch.std(preds, dim=0).mean(dim=-1).sum() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "        avg_std += torch.std(preds.softmax(-1), dim=0).mean(dim=-1).sum() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "        _, all_preds = preds.max(-1)\n",
    "        ag_p, dag_p, ag_c_p, ag_w_p, dag_w_p, dag_c_p = 0, 0, 0, 0, 0, 0\n",
    "        kld = 0.\n",
    "        for p in pairs:\n",
    "            ag, dag, ag_c, ag_w, dag_w, dag_c = compute_pair_consensus(all_preds[p,:])\n",
    "            ag_p += ag\n",
    "            dag_p += dag\n",
    "            ag_c_p += ag_c\n",
    "            ag_w_p += ag_w\n",
    "            dag_c_p += dag_c\n",
    "            dag_w_p += dag_w\n",
    "            kld += kl_div(preds[p[0]], preds[p[1]])\n",
    "        \n",
    "        ag_sum += ag_p/len(pairs)\n",
    "        dag_sum += dag_p/len(pairs)\n",
    "        ag_c_sum += ag_c_p/len(pairs)\n",
    "        ag_w_sum += ag_w_p/len(pairs)\n",
    "        dag_c_sum += dag_c_p/len(pairs)\n",
    "        dag_w_sum += dag_w_p/len(pairs)\n",
    "        kld_sum += kld/len(pairs)\n",
    "        \n",
    "        label_matrix = (all_preds == target).float().T\n",
    "        logit = label_matrix.T.unsqueeze(2).repeat(1,1,1000) * preds.softmax(dim=-1)\n",
    "        weighted_ensem = logit.sum(dim=0)\n",
    "        naive_ensem = preds.softmax(dim=-1).mean(dim=0)\n",
    "        _, w_ensem_pred = weighted_ensem.max(-1)\n",
    "        _, n_ensem_pred = naive_ensem.max(-1)\n",
    "        w_acc += (w_ensem_pred == target).sum()\n",
    "        n_acc += (n_ensem_pred == target).sum()\n",
    "        \n",
    "        ece1 += ece_criterion(output1.softmax(-1), target)\n",
    "        \n",
    "    targets.append(label_matrix)\n",
    "print(f\"UB: {w_acc/len(test_dataset)} | ensem: {n_acc/len(test_dataset)}\")\n",
    "print(f\"agree: {ag_sum/len(test_dataset)} | disagree: {dag_sum/len(test_dataset)}\") \n",
    "print(f\"agree_correct: {ag_c_sum/len(test_dataset)} | agree_wrong: {ag_w_sum/len(test_dataset)}\") \n",
    "print(f\"disagree_1correct: {dag_c_sum/len(test_dataset)} | disagree_2wrong: {dag_w_sum/len(test_dataset)}\") \n",
    "print(f\"Ensemble Variance Logits: {avg_std_logits/len(test_dataset)}\") \n",
    "print(f\"Ensemble Variance: {avg_std/len(test_dataset)}\") \n",
    "print(f\"KL div: {kld_sum/len(test_dataset)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-congo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
