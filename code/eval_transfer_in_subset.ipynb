{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "female-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets, transforms\n",
    "from datasets import Split_Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder, CIFAR10, CIFAR100\n",
    "from datasets_v08 import Flowers102\n",
    "from datasets import INaturalist, Split_Dataset\n",
    "import os\n",
    "\n",
    "in_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "num_classes = 100\n",
    "val_perc = 100\n",
    "\n",
    "in_val_transforms = transforms.Compose([\n",
    "        transforms.Resize(224+32),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        in_norm\n",
    "    ])\n",
    "\n",
    "in_dataset = Split_Dataset('/data/scratch/swhan/data/imagenet/',  \\\n",
    "                            f'./calib_splits/am_IN{val_perc}_val.txt',\n",
    "                            transform=in_val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "realistic-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_loader = torch.utils.data.DataLoader(\n",
    "            in_dataset, batch_size=256, shuffle=False,\n",
    "            num_workers=16, pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriented-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_1_model(ckpt_path, full_path=False, num_classes=1000):\n",
    "    model1 = models.resnet50(num_classes=num_classes).cuda()\n",
    "    if not full_path:\n",
    "        sd = torch.load(f\"./dist_models/{ckpt_path}/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "    else:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    ckpt = {k.replace(\"members.0.\",\"\"):v for k,v in sd['model'].items()}\n",
    "    model1.load_state_dict(ckpt)\n",
    "    print(f\"loaded {ckpt_path}\")\n",
    "    model1.eval()\n",
    "    return model1\n",
    "\n",
    "def rollout_loader(model, loader):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    for it, (img, target) in enumerate(loader):\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        img = img.cuda(non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            output1 = model(img)\n",
    "#             ece_1 = ece_criterion(output1.softmax(-1), target)\n",
    "            targets.append(target)\n",
    "            outputs.append(output1)\n",
    "    return torch.cat(outputs), torch.cat(targets)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import inspect\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "cecriterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "nll_criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "# ece_criterion = _ECELoss().cuda()\n",
    "ece_netcal = ECE(15)\n",
    "\n",
    "def get_metrics(outs, tars, names, printing=True, input_softmax=False, num_classes=1000):\n",
    "\n",
    "    for out, tar,name in zip(outs,tars,names):\n",
    "        correct_per_class = torch.zeros(num_classes).to(tar.device)\n",
    "        total_per_class = torch.zeros(num_classes).to(tar.device)\n",
    "\n",
    "        if not input_softmax:\n",
    "            out = out.softmax(-1)\n",
    "        ece1 = ece_netcal.measure(out.cpu().numpy(), tar.cpu().numpy())\n",
    "#         ece2 = ece_criterion(out, tar)\n",
    "        loss = F.nll_loss(torch.log(out), tar)\n",
    "        _, pred = out.max(-1)\n",
    "        correct_vec = (pred == tar)\n",
    "        ind_per_class = (tar.unsqueeze(1) == torch.arange(num_classes).to(tar.device)) # indicator variable for each class\n",
    "        correct_per_class = (correct_vec.unsqueeze(1) * ind_per_class).sum(0)\n",
    "        total_per_class = ind_per_class.sum(0)\n",
    "\n",
    "        acc = (correct_vec.sum()) / len(tar)\n",
    "        acc_per_class = correct_per_class / total_per_class\n",
    "        if printing:\n",
    "            print(name)\n",
    "            print(f\"NLL: {loss.item()} | ECE: {ece1}\")\n",
    "            print(\"Acc:\", acc.item())\n",
    "    return loss.item(), ece1, acc.item(), acc_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLD, self).__init__()\n",
    "        self.kl = torch.nn.KLDivLoss(reduction='sum', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.tensor, q: torch.tensor):\n",
    "        p = F.log_softmax(p, dim=-1)\n",
    "        q = F.log_softmax(q, dim=-1)\n",
    "        return self.kl(p,q)\n",
    "\n",
    "kl_div = KLD()\n",
    "\n",
    "def compute_pair_consensus(pair_preds, target):\n",
    "    agree = (pair_preds[0] == pair_preds[1])\n",
    "    agree_correct = agree & (pair_preds[0] == target)\n",
    "    agree_wrong = agree & (pair_preds[0] != target)\n",
    "    disagree = (pair_preds[0] != pair_preds[1])\n",
    "    disagree_both_wrong = disagree & (pair_preds[0] != target) & (pair_preds[1] != target)\n",
    "    disagree_one_correct = disagree & (pair_preds[0] != target) & (pair_preds[1] == target) \n",
    "    disagree_one_correct2 = disagree & (pair_preds[1] != target) & (pair_preds[0] == target) \n",
    "    return agree.sum(), disagree.sum(), agree_correct.sum(), agree_wrong.sum(), disagree_both_wrong.sum(), disagree_one_correct.sum()+disagree_one_correct2.sum()\n",
    "\n",
    "def get_div_metrics(output1,output2,output3,target):\n",
    "    preds = torch.stack([output1,output2,output3])\n",
    "    avg_std_logits = torch.std(preds, dim=0).mean(dim=-1).mean() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "    avg_std = torch.std(preds.softmax(-1), dim=0).mean(dim=-1).mean() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "    _, all_preds = preds.max(-1)\n",
    "    ag_p, dag_p, ag_c_p, ag_w_p, dag_w_p, dag_c_p = 0, 0, 0, 0, 0, 0\n",
    "    kld = 0.\n",
    "    pairs = ([0,1], [0,2], [1,2])\n",
    "    for p in pairs:\n",
    "        ag, dag, ag_c, ag_w, dag_w, dag_c = compute_pair_consensus(all_preds[p,:], target)\n",
    "        ag_p += ag\n",
    "        dag_p += dag\n",
    "        ag_c_p += ag_c\n",
    "        ag_w_p += ag_w\n",
    "        dag_c_p += dag_c\n",
    "        dag_w_p += dag_w\n",
    "        kld += kl_div(preds[p[0]], preds[p[1]])\n",
    "\n",
    "    ag_sum = ag_p/len(pairs)\n",
    "    dag_sum = dag_p/len(pairs)\n",
    "    ag_c_sum = ag_c_p/len(pairs)\n",
    "    ag_w_sum = ag_w_p/len(pairs)\n",
    "    dag_c_sum = dag_c_p/len(pairs)\n",
    "    dag_w_sum = dag_w_p/len(pairs)\n",
    "    kld_sum = kld/len(pairs)\n",
    "    print(f\"Diversity agree: {ag_sum/len(output1)} | disagree: {dag_sum/len(output1)}\") \n",
    "#     print(f\"Ensemble Variance Logits: {avg_std_logits} \") \n",
    "#     print(f\"Ensemble Variance: {avg_std}\") \n",
    "#     print(f\"KL div: {kld_sum/len(output1)}\") \n",
    "    return ag_sum/len(output1), dag_sum/len(output1), kld_sum/len(output1), avg_std_logits, avg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convertible-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get classwise stats\n",
    "def get_classwise(acc_base, acc_rotinv, acc_roteq, num_classes=1000):\n",
    "    print(\"use order B, I, E\")\n",
    "    y = torch.stack([v for v in [acc_base, acc_rotinv, acc_roteq]], dim=-1)\n",
    "\n",
    "    fac = num_classes/100\n",
    "    # all 3 models equally good\n",
    "    best_base_inv_eq = (y[:,0] == y[:,1]) & (y[:,1] == y[:,2])\n",
    "    # 2 models equally good and is better\n",
    "    best_base_inv = (y[:,0] == y[:,1]) & (y[:,0] > y[:,2])\n",
    "    best_base_eq = (y[:,0] == y[:,2]) & (y[:,0] > y[:,1])\n",
    "    best_inv_eq = (y[:,1] == y[:,2]) & (y[:,1] > y[:,0])\n",
    "    # 2 models equally good and is worse\n",
    "    worse_base_inv = (y[:,0] == y[:,1]) & (y[:,0] < y[:,2]) # best eq\n",
    "    worse_base_eq = (y[:,0] == y[:,2]) & (y[:,0] < y[:,1]) # best inv\n",
    "    worse_inv_eq = (y[:,1] == y[:,2]) & (y[:,1] < y[:,0]) # best base\n",
    "    all_diff = (y[:,0] != y[:,1]) & (y[:,1] != y[:,2]) & (y[:,0] != y[:,2])\n",
    "    print(f\"all equal best: {(best_base_inv_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"B,I equal best: {(best_base_inv.sum())/fac:.1f}%\")\n",
    "    print(f\"B,E equal best: {(best_base_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"I,E equal best: {(best_inv_eq.sum())/fac:.1f}%\")\n",
    "    # print(f\"all diff: {all_diff.sum()}\")\n",
    "    total = best_base_inv_eq.sum() + best_base_inv.sum() + best_base_eq.sum() + best_inv_eq.sum() + all_diff.sum() + worse_inv_eq.sum() + worse_base_eq.sum() + worse_base_inv.sum()\n",
    "    assert total == num_classes\n",
    "\n",
    "    # for all diff \n",
    "    best_base = (y[:,0] > y[:,1]) & (y[:,0] > y[:,2]) & all_diff\n",
    "    best_inv = (y[:,1] > y[:,0]) & (y[:,1] > y[:,2]) & all_diff\n",
    "    best_eq = (y[:,2] > y[:,0]) & (y[:,2] > y[:,1]) & all_diff\n",
    "    total_unique = best_base.sum()+best_inv.sum()+best_eq.sum()\n",
    "    assert total_unique == all_diff.sum()\n",
    "\n",
    "    # single model uniquely best\n",
    "    b_uniq = best_base | worse_inv_eq\n",
    "    i_uniq = best_inv | worse_base_eq\n",
    "    e_uniq = best_eq | worse_base_inv\n",
    "    print(f\"B uniquely best: {b_uniq.sum()/fac:.1f}%\")\n",
    "    print(f\"I uniquely best: {(best_inv.sum() + worse_base_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"E uniquely best: {(best_eq.sum() + worse_base_inv.sum())/fac:.1f}%\")\n",
    "\n",
    "    B_good = b_uniq | best_base_inv_eq | best_base_inv | best_base_eq\n",
    "    I_good = i_uniq | best_base_inv_eq | best_base_inv | best_inv_eq\n",
    "    E_good = e_uniq | best_base_inv_eq | best_base_eq | best_inv_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupied-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ./checkpoints/in1000-in100-base-seed31-lp-lr0.3-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/in1000-in100-roteq-seed24-lp-lr0.3-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/in1000-in100-roteq-seed69-lp-lr0.3-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/in1000-in100-rotinv-seed24-lp-lr0.3-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/in1000-in100-rotinv-seed31-lp-lr0.3-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/in1000-in100-rotinv-seed69-lp-lr0.3-cosine/checkpoint_best.pth\n"
     ]
    }
   ],
   "source": [
    "# INat\n",
    "dataset_name = 'imagenet'\n",
    "num_classes = 100\n",
    "lr = '0.3'\n",
    "loader = in_loader\n",
    "\n",
    "b31 = load_1_model('./checkpoints/in1000-in100-base-seed31-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "e24 = load_1_model('./checkpoints/in1000-in100-roteq-seed24-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "# e31 = load_1_model('./checkpoints/inat-roteq-base31-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "e69 = load_1_model('./checkpoints/in1000-in100-roteq-seed69-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "i24 = load_1_model('./checkpoints/in1000-in100-rotinv-seed24-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "i31 = load_1_model('./checkpoints/in1000-in100-rotinv-seed31-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "i69 = load_1_model('./checkpoints/in1000-in100-rotinv-seed69-lp-lr0.3-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "b31_out_in, b31_tar = rollout_loader(b31, loader)\n",
    "e24_out_in, e24_tar = rollout_loader(e24, loader)\n",
    "# e31_out_in, e31_tar = rollout_loader(e31, loader)\n",
    "e69_out_in, e69_tar = rollout_loader(e69, loader)\n",
    "i24_out_in, i24_tar = rollout_loader(i24, loader)\n",
    "i31_out_in, i31_tar = rollout_loader(i31, loader)\n",
    "i69_out_in, i69_tar = rollout_loader(i69, loader)\n",
    "\n",
    "assert(torch.equal(b31_tar, e24_tar))\n",
    "\n",
    "tar_in = b31_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "owned-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metrics import get_metrics as get_new_metrics\n",
    "\n",
    "def ensem_BEI(all_eq,all_base,all_inv,same_tar,num_E=0, num_B=0, num_I=0, num_comb=5, err='std'):\n",
    "    ee_nll = []\n",
    "    ee_ece = []\n",
    "    ee_acc = []\n",
    "    for i in range(num_comb):\n",
    "        eq_list = np.random.choice(all_eq, num_E, replace=False)\n",
    "        base_list = np.random.choice(all_base, num_B, replace=False)\n",
    "        inv_list = np.random.choice(all_inv, num_I, replace=False)\n",
    "        out_list = list(eq_list) + list(base_list) + list(inv_list)\n",
    "        out_list = [torch.Tensor(x.cpu()) for x in out_list]\n",
    "        ee_out = torch.stack(out_list).softmax(-1).mean(dim=0).cuda()\n",
    "        nll, ece, acc, _ = get_new_metrics([ee_out],[same_tar],[f'EE_comb{i}'], printing=False, input_softmax=True)    \n",
    "        ee_nll.append(nll)\n",
    "        ee_ece.append(ece)  \n",
    "        ee_acc.append(acc)\n",
    "    print(\"E\"*num_E + \"B\"*num_B + \"I\"*num_I)\n",
    "    if err=='std':\n",
    "        print(f\"NLL: {np.mean(ee_nll):.4f} +/- {np.std(ee_nll):.4f}\")\n",
    "        print(f\"ECE: {np.mean(ee_ece):.4f} +/- {np.std(ee_ece):.4f}\")\n",
    "        print(f\"Acc: {np.mean(ee_acc):.4f} +/- {np.std(ee_acc):.4f}\")\n",
    "    elif err=='var':\n",
    "        print(f\"NLL: {np.mean(ee_nll):.4f} +/- {np.var(ee_nll):.4f}\")\n",
    "        print(f\"ECE: {np.mean(ee_ece):.4f} +/- {np.var(ee_ece):.4f}\")\n",
    "        print(f\"Acc: {np.mean(ee_acc):.4f} +/- {np.var(ee_acc):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-silence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "NLL: 0.3132 +/- 0.0000\n",
      "ECE: 0.0109 +/- 0.0000\n",
      "Acc: 0.9122 +/- 0.0000\n",
      "B\n",
      "NLL: 0.3149 +/- 0.0000\n",
      "ECE: 0.0114 +/- 0.0000\n",
      "Acc: 0.9134 +/- 0.0000\n",
      "I\n",
      "NLL: 0.3537 +/- 0.0030\n",
      "ECE: 0.0123 +/- 0.0033\n",
      "Acc: 0.9032 +/- 0.0009\n"
     ]
    }
   ],
   "source": [
    "# 1 models\n",
    "all_eq_f = [e69_out_in.cpu(), e24_out_in.cpu()]\n",
    "all_base_f = [b31_out_in.cpu(), b31_out_in.cpu()] #duplicated to prevent err\n",
    "all_inv_f = [i69_out_in.cpu(), i31_out_in.cpu(), i24_out_in.cpu()] #duplicated to prevent err\n",
    "\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_E=1, num_comb=2)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_B=1, num_comb=2)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_I=1, num_comb=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ec98d-28fd-46bf-9051-084a673d398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 models\n",
    "all_eq_f = [e69_out_inat.cpu(), e69_out_inat.cpu()]\n",
    "\n",
    "\n",
    "\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=4)\n",
    "\n",
    "all_eq_f = [e31_out_inat.cpu(),e31_out_inat.cpu()]\n",
    "\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=4)\n",
    "\n",
    "all_eq_f = [e24_out_inat.cpu(), e24_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=4)\n",
    "\n",
    "\n",
    "all_eq_f = [i31_out_inat.cpu(), i31_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dying-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EE\n",
      "NLL: 0.2877 +/- 0.0000\n",
      "ECE: 0.0114 +/- 0.0000\n",
      "Acc: 0.9166 +/- 0.0000\n",
      "EI\n",
      "NLL: 0.2964 +/- 0.0012\n",
      "ECE: 0.0204 +/- 0.0012\n",
      "Acc: 0.9174 +/- 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21263/2879841140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 3 models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mensem_BEI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_eq_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_base_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inv_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtar_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_E\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mensem_BEI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_eq_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_base_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inv_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtar_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_E\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_I\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21263/1395969937.py\u001b[0m in \u001b[0;36mensem_BEI\u001b[0;34m(all_eq, all_base, all_inv, same_tar, num_E, num_B, num_I, num_comb, err)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mee_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_comb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0meq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_eq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mbase_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minv_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# 2 models\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_E=2)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_E=1, num_I=1)\n",
    "\n",
    "# 3 models\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_E=3)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_in,num_E=2, num_I=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tar = b69_tar\n",
    "num_comb = 1\n",
    "all_eq_exR = [eq24_out, eq69_out, eq31_out]\n",
    "all_base_exR = [b24_out, b69_out, b31_out]\n",
    "all_inv_exR = [inv24_out, inv69_out, inv31_out]\n",
    "for i in range(num_comb):\n",
    "    [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "    [b1] = np.random.choice(all_base_exR, 1)\n",
    "    [inv1] = np.random.choice(all_inv_exR, 1) \n",
    "    _,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "    get_classwise(acc_pc_b, acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100\n",
    "dataset_name = 'cifar100'\n",
    "num_classes = 100\n",
    "lr = '0.2'\n",
    "loader = c100_loader\n",
    "\n",
    "b69 = load_1_model(f\"trans_{dataset_name}_base69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b69_out, b69_tar = rollout_loader(b69, loader)\n",
    "same_tar = b69_tar\n",
    "get_metrics([b69_out], [same_tar],['b69'], num_classes=num_classes)\n",
    "eq69 = load_1_model(f\"trans_{dataset_name}_eq69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq69_out, eq69_tar = rollout_loader(eq69, loader)\n",
    "get_metrics([eq69_out], [same_tar],['eq69'], num_classes=num_classes)\n",
    "\n",
    "inv69 = load_1_model(f\"trans_{dataset_name}_inv69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv69_out, inv69_tar = rollout_loader(inv69, loader)\n",
    "get_metrics([inv69_out], [same_tar],['inv69'], num_classes=num_classes)\n",
    "\n",
    "assert(torch.equal(b69_tar,eq69_tar))\n",
    "\n",
    "b24 = load_1_model(f\"trans_{dataset_name}_base24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b24_out, _ = rollout_loader(b24, loader)\n",
    "get_metrics([b24_out], [same_tar],['b24'], num_classes=num_classes)\n",
    "\n",
    "eq24 = load_1_model(f\"trans_{dataset_name}_eq24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq24_out, _ = rollout_loader(eq24, loader)\n",
    "get_metrics([eq24_out], [same_tar],['eq24'], num_classes=num_classes)\n",
    "\n",
    "inv24 = load_1_model(f\"trans_{dataset_name}_inv24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv24_out, _ = rollout_loader(inv24, loader)\n",
    "get_metrics([inv24_out], [same_tar],['inv24'], num_classes=num_classes)\n",
    "\n",
    "\n",
    "b31 = load_1_model(f\"trans_{dataset_name}_base31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b31_out, _ = rollout_loader(b31, loader)\n",
    "get_metrics([b31_out], [same_tar],['b31'], num_classes=num_classes)\n",
    "\n",
    "eq31 = load_1_model(f\"trans_{dataset_name}_eq31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq31_out, _ = rollout_loader(eq31, loader)\n",
    "get_metrics([eq31_out], [same_tar],['eq31'], num_classes=num_classes)\n",
    "\n",
    "inv31 = load_1_model(f\"trans_{dataset_name}_inv31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv31_out, _ = rollout_loader(inv31, loader)\n",
    "get_metrics([inv31_out], [same_tar],['inv31'], num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar100'\n",
    "num_classes = 100\n",
    "lr = '0.2'\n",
    "loader = c100_loader\n",
    "\n",
    "eq42 = load_1_model(f\"trans_{dataset_name}_eq42_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq42_out, eq42_tar = rollout_loader(eq42, loader)\n",
    "get_metrics([eq42_out], [eq42_tar],['eq42'], num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tar = eq42_tar\n",
    "all_eq = [eq69_out, eq24_out, eq31_out, eq42_out]\n",
    "all_base = [b69_out, b24_out, b31_out]\n",
    "all_inv = [inv69_out, inv24_out, inv31_out]\n",
    "\n",
    "# 1 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar, num_E=1)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_B=1)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_I=1)\n",
    "\n",
    "# 2 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=2)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=1, num_I=1)\n",
    "\n",
    "# 3 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=3)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=2, num_I=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "(91.2+91.9)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "(91.2+91.9+91.9)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "87.03-(85.5+84.0+85.5)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "86.54-(85.5+84.0)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = b24_out\n",
    "eq1 = eq42_out\n",
    "inv1 = inv69_out\n",
    "\n",
    "_,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "_,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "_,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "get_classwise(acc_pc_b, acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tar = b69_tar\n",
    "num_comb = 1\n",
    "all_eq_exR = [eq24_out, eq69_out, eq31_out]\n",
    "all_base_exR = [b24_out, b69_out, b31_out]\n",
    "all_inv_exR = [inv24_out, inv69_out, inv31_out]\n",
    "for i in range(num_comb):\n",
    "    [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "    [b1] = np.random.choice(all_base_exR, 1)\n",
    "    [inv1] = np.random.choice(all_inv_exR, 1) \n",
    "    _,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "    get_classwise(acc_pc_b, acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comb =1 \n",
    "eee_nll = []\n",
    "eee_ece = []\n",
    "eee_acc = []\n",
    "for i in range(num_comb):\n",
    "    [eq1, eq2, eq3] = np.random.choice(all_eq_exR, 3, replace=False)\n",
    "    eee_out = (eq1.softmax(-1) + eq2.softmax(-1) + eq3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([eee_out],[same_tar],[f'EEE_comb{i}'], input_softmax=True, num_classes=num_classes)    \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(eq1,eq2,eq3, same_tar)\n",
    "\n",
    "    eee_nll.append(nll)\n",
    "    eee_ece.append(ece)  \n",
    "    eee_acc.append(acc)   \n",
    "\n",
    "bbb_nll = []\n",
    "bbb_ece = []\n",
    "bbb_acc = []\n",
    "for i in range(num_comb):\n",
    "    [b1, b2, b3] = np.random.choice(all_base_exR, 3, replace=False)\n",
    "    bbb_out = (b1.softmax(-1) + b2.softmax(-1) + b3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([bbb_out],[same_tar],[f'BBB_comb{i}'], input_softmax=True, num_classes=num_classes)      \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(b1,b2,b3, same_tar)\n",
    "\n",
    "    bbb_nll.append(nll)\n",
    "    bbb_ece.append(ece)  \n",
    "    bbb_acc.append(acc) \n",
    "\n",
    "iii_nll = []\n",
    "iii_ece = []\n",
    "iii_acc = []\n",
    "for i in range(num_comb):\n",
    "    [i1, i2, i3] = np.random.choice(all_inv_exR, 3, replace=False)\n",
    "    iii_out = (i1.softmax(-1) + i2.softmax(-1) + i3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([iii_out],[same_tar],[f'III_comb{i}'], input_softmax=True, num_classes=num_classes)      \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(i1,i2,i3, same_tar)\n",
    "    \n",
    "    iii_nll.append(nll)\n",
    "    iii_ece.append(ece)  \n",
    "    iii_acc.append(acc) \n",
    "    \n",
    "bei_nll = []\n",
    "bei_ece = []\n",
    "bei_acc = []\n",
    "for i in range(num_comb):\n",
    "#     [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "#     [base1] = np.random.choice(all_base_exR, 1)\n",
    "#     [inv1] = np.random.choice(all_inv_exR, 1)\n",
    "    [eq1] = np.random.choice([eq31_out], 1)\n",
    "    [base1] = np.random.choice([b31_out], 1)\n",
    "    [inv1] = np.random.choice([inv24_out], 1)\n",
    "    bei_out = (eq1.softmax(-1) + base1.softmax(-1) + inv1.softmax(-1))/3\n",
    "    \n",
    "    nll, ece, acc, _ = get_metrics([bei_out],[same_tar],[f'BEI_comb{i}'], input_softmax=True, num_classes=num_classes)     \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(eq1,base1,inv1, same_tar)\n",
    "    \n",
    "    bei_nll.append(nll)\n",
    "    bei_ece.append(ece)  \n",
    "    bei_acc.append(acc) \n",
    "\n",
    "print(f\"\\nEEE Acc: {np.mean(eee_acc)} +/- {np.std(eee_acc)}\")\n",
    "print(f\"EEE ECE: {np.mean(eee_ece)} +/- {np.std(eee_ece)}\")\n",
    "print(f\"EEE NLL: {np.mean(eee_nll)} +/- {np.std(eee_nll)}\")\n",
    "\n",
    "print(f\"\\nBBB Acc: {np.mean(bbb_acc)} +/- {np.std(bbb_acc)}\")\n",
    "print(f\"BBB ECE: {np.mean(bbb_ece)} +/- {np.std(bbb_ece)}\")\n",
    "print(f\"BBB NLL: {np.mean(bbb_nll)} +/- {np.std(bbb_nll)}\")\n",
    "\n",
    "print(f\"\\nIII Acc: {np.mean(iii_acc)} +/- {np.std(iii_acc)}\")\n",
    "print(f\"III ECE: {np.mean(iii_ece)} +/- {np.std(iii_ece)}\")\n",
    "print(f\"III NLL: {np.mean(iii_nll)} +/- {np.std(iii_nll)}\")\n",
    "\n",
    "print(f\"\\nBEI Acc: {np.mean(bei_acc)} +/- {np.std(bei_acc)}\")\n",
    "print(f\"BEI ECE: {np.mean(bei_ece)} +/- {np.std(bei_ece)}\")\n",
    "print(f\"BEI NLL: {np.mean(bei_nll)} +/- {np.std(bei_nll)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(b24_out,eq24_out,inv24_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(eq69_out,eq24_out,eq31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(inv69_out,inv24_out,inv31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(b69_out,b24_out,b31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-tunisia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
