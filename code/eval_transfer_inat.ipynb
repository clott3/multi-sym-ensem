{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "female-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets, transforms\n",
    "from datasets import Split_Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder, CIFAR10, CIFAR100\n",
    "from datasets_v08 import Flowers102\n",
    "from datasets import INaturalist\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device('cuda:4')\n",
    "\n",
    "inat_norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "num_classes = 1010\n",
    "\n",
    "inat_val_transforms = transforms.Compose([\n",
    "        transforms.Resize(224+32),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        inat_norm\n",
    "    ])\n",
    "\n",
    "inat_dataset = INaturalist('/data/scratch/swhan/data/inat-1k/', version='2019', transform=inat_val_transforms)\n",
    "split_idx_path = './misc/inat-1k-train-val-split-idx.pth'\n",
    "\n",
    "# creating/loading train-val (90 - 10) split\n",
    "if os.path.exists(split_idx_path):\n",
    "    split_idx = torch.load(split_idx_path)\n",
    "inat_dataset = Subset(inat_dataset, split_idx['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "realistic-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "inat_loader = torch.utils.data.DataLoader(\n",
    "            inat_dataset, batch_size=256, shuffle=False,\n",
    "            num_workers=16, pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriented-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_1_model(ckpt_path, full_path=False, num_classes=1000):\n",
    "    model1 = models.resnet50(num_classes=num_classes).cuda()\n",
    "    if not full_path:\n",
    "        sd = torch.load(f\"./dist_models/{ckpt_path}/checkpoint_best.pth\", map_location=\"cpu\")\n",
    "    else:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    ckpt = {k.replace(\"members.0.\",\"\"):v for k,v in sd['model'].items()}\n",
    "    model1.load_state_dict(ckpt)\n",
    "    print(f\"loaded {ckpt_path}\")\n",
    "    model1.eval()\n",
    "    return model1\n",
    "\n",
    "def rollout_loader(model, loader):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    for it, (img, target) in enumerate(loader):\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        img = img.cuda(non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            output1 = model(img)\n",
    "#             ece_1 = ece_criterion(output1.softmax(-1), target)\n",
    "            targets.append(target)\n",
    "            outputs.append(output1)\n",
    "    return torch.cat(outputs), torch.cat(targets)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import inspect\n",
    "from netcal.metrics import ECE\n",
    "\n",
    "cecriterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "nll_criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "# ece_criterion = _ECELoss().cuda()\n",
    "ece_netcal = ECE(15)\n",
    "\n",
    "def get_metrics(outs, tars, names, printing=True, input_softmax=False, num_classes=1000):\n",
    "\n",
    "    for out, tar,name in zip(outs,tars,names):\n",
    "        correct_per_class = torch.zeros(num_classes).to(tar.device)\n",
    "        total_per_class = torch.zeros(num_classes).to(tar.device)\n",
    "\n",
    "        if not input_softmax:\n",
    "            out = out.softmax(-1)\n",
    "        ece1 = ece_netcal.measure(out.cpu().numpy(), tar.cpu().numpy())\n",
    "#         ece2 = ece_criterion(out, tar)\n",
    "        loss = F.nll_loss(torch.log(out), tar)\n",
    "        _, pred = out.max(-1)\n",
    "        correct_vec = (pred == tar)\n",
    "        ind_per_class = (tar.unsqueeze(1) == torch.arange(num_classes).to(tar.device)) # indicator variable for each class\n",
    "        correct_per_class = (correct_vec.unsqueeze(1) * ind_per_class).sum(0)\n",
    "        total_per_class = ind_per_class.sum(0)\n",
    "\n",
    "        acc = (correct_vec.sum()) / len(tar)\n",
    "        acc_per_class = correct_per_class / total_per_class\n",
    "        if printing:\n",
    "            print(name)\n",
    "            print(f\"NLL: {loss.item()} | ECE: {ece1}\")\n",
    "            print(\"Acc:\", acc.item())\n",
    "    return loss.item(), ece1, acc.item(), acc_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sized-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLD, self).__init__()\n",
    "        self.kl = torch.nn.KLDivLoss(reduction='sum', log_target=True)\n",
    "\n",
    "    def forward(self, p: torch.tensor, q: torch.tensor):\n",
    "        p = F.log_softmax(p, dim=-1)\n",
    "        q = F.log_softmax(q, dim=-1)\n",
    "        return self.kl(p,q)\n",
    "\n",
    "kl_div = KLD()\n",
    "\n",
    "def compute_pair_consensus(pair_preds, target):\n",
    "    agree = (pair_preds[0] == pair_preds[1])\n",
    "    agree_correct = agree & (pair_preds[0] == target)\n",
    "    agree_wrong = agree & (pair_preds[0] != target)\n",
    "    disagree = (pair_preds[0] != pair_preds[1])\n",
    "    disagree_both_wrong = disagree & (pair_preds[0] != target) & (pair_preds[1] != target)\n",
    "    disagree_one_correct = disagree & (pair_preds[0] != target) & (pair_preds[1] == target) \n",
    "    disagree_one_correct2 = disagree & (pair_preds[1] != target) & (pair_preds[0] == target) \n",
    "    return agree.sum(), disagree.sum(), agree_correct.sum(), agree_wrong.sum(), disagree_both_wrong.sum(), disagree_one_correct.sum()+disagree_one_correct2.sum()\n",
    "\n",
    "def get_div_metrics(output1,output2,output3,target):\n",
    "    preds = torch.stack([output1,output2,output3])\n",
    "    avg_std_logits = torch.std(preds, dim=0).mean(dim=-1).mean() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "    avg_std = torch.std(preds.softmax(-1), dim=0).mean(dim=-1).mean() # std over members, mean over classes, sum over samples (mean taken later))\n",
    "    _, all_preds = preds.max(-1)\n",
    "    ag_p, dag_p, ag_c_p, ag_w_p, dag_w_p, dag_c_p = 0, 0, 0, 0, 0, 0\n",
    "    kld = 0.\n",
    "    pairs = ([0,1], [0,2], [1,2])\n",
    "    for p in pairs:\n",
    "        ag, dag, ag_c, ag_w, dag_w, dag_c = compute_pair_consensus(all_preds[p,:], target)\n",
    "        ag_p += ag\n",
    "        dag_p += dag\n",
    "        ag_c_p += ag_c\n",
    "        ag_w_p += ag_w\n",
    "        dag_c_p += dag_c\n",
    "        dag_w_p += dag_w\n",
    "        kld += kl_div(preds[p[0]], preds[p[1]])\n",
    "\n",
    "    ag_sum = ag_p/len(pairs)\n",
    "    dag_sum = dag_p/len(pairs)\n",
    "    ag_c_sum = ag_c_p/len(pairs)\n",
    "    ag_w_sum = ag_w_p/len(pairs)\n",
    "    dag_c_sum = dag_c_p/len(pairs)\n",
    "    dag_w_sum = dag_w_p/len(pairs)\n",
    "    kld_sum = kld/len(pairs)\n",
    "    print(f\"Diversity agree: {ag_sum/len(output1)} | disagree: {dag_sum/len(output1)}\") \n",
    "#     print(f\"Ensemble Variance Logits: {avg_std_logits} \") \n",
    "#     print(f\"Ensemble Variance: {avg_std}\") \n",
    "#     print(f\"KL div: {kld_sum/len(output1)}\") \n",
    "    return ag_sum/len(output1), dag_sum/len(output1), kld_sum/len(output1), avg_std_logits, avg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convertible-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get classwise stats\n",
    "def get_classwise(acc_base, acc_rotinv, acc_roteq, num_classes=1000):\n",
    "    print(\"use order B, I, E\")\n",
    "    y = torch.stack([v for v in [acc_base, acc_rotinv, acc_roteq]], dim=-1)\n",
    "\n",
    "    fac = num_classes/100\n",
    "    # all 3 models equally good\n",
    "    best_base_inv_eq = (y[:,0] == y[:,1]) & (y[:,1] == y[:,2])\n",
    "    # 2 models equally good and is better\n",
    "    best_base_inv = (y[:,0] == y[:,1]) & (y[:,0] > y[:,2])\n",
    "    best_base_eq = (y[:,0] == y[:,2]) & (y[:,0] > y[:,1])\n",
    "    best_inv_eq = (y[:,1] == y[:,2]) & (y[:,1] > y[:,0])\n",
    "    # 2 models equally good and is worse\n",
    "    worse_base_inv = (y[:,0] == y[:,1]) & (y[:,0] < y[:,2]) # best eq\n",
    "    worse_base_eq = (y[:,0] == y[:,2]) & (y[:,0] < y[:,1]) # best inv\n",
    "    worse_inv_eq = (y[:,1] == y[:,2]) & (y[:,1] < y[:,0]) # best base\n",
    "    all_diff = (y[:,0] != y[:,1]) & (y[:,1] != y[:,2]) & (y[:,0] != y[:,2])\n",
    "    print(f\"all equal best: {(best_base_inv_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"B,I equal best: {(best_base_inv.sum())/fac:.1f}%\")\n",
    "    print(f\"B,E equal best: {(best_base_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"I,E equal best: {(best_inv_eq.sum())/fac:.1f}%\")\n",
    "    # print(f\"all diff: {all_diff.sum()}\")\n",
    "    total = best_base_inv_eq.sum() + best_base_inv.sum() + best_base_eq.sum() + best_inv_eq.sum() + all_diff.sum() + worse_inv_eq.sum() + worse_base_eq.sum() + worse_base_inv.sum()\n",
    "    assert total == num_classes\n",
    "\n",
    "    # for all diff \n",
    "    best_base = (y[:,0] > y[:,1]) & (y[:,0] > y[:,2]) & all_diff\n",
    "    best_inv = (y[:,1] > y[:,0]) & (y[:,1] > y[:,2]) & all_diff\n",
    "    best_eq = (y[:,2] > y[:,0]) & (y[:,2] > y[:,1]) & all_diff\n",
    "    total_unique = best_base.sum()+best_inv.sum()+best_eq.sum()\n",
    "    assert total_unique == all_diff.sum()\n",
    "\n",
    "    # single model uniquely best\n",
    "    b_uniq = best_base | worse_inv_eq\n",
    "    i_uniq = best_inv | worse_base_eq\n",
    "    e_uniq = best_eq | worse_base_inv\n",
    "    print(f\"B uniquely best: {b_uniq.sum()/fac:.1f}%\")\n",
    "    print(f\"I uniquely best: {(best_inv.sum() + worse_base_eq.sum())/fac:.1f}%\")\n",
    "    print(f\"E uniquely best: {(best_eq.sum() + worse_base_inv.sum())/fac:.1f}%\")\n",
    "\n",
    "    B_good = b_uniq | best_base_inv_eq | best_base_inv | best_base_eq\n",
    "    I_good = i_uniq | best_base_inv_eq | best_base_inv | best_inv_eq\n",
    "    E_good = e_uniq | best_base_inv_eq | best_base_eq | best_inv_eq\n",
    "    \n",
    "def get_classwise_ei(acc_rotinv, acc_roteq, num_classes=1000):\n",
    "    y = torch.stack([v for v in [acc_rotinv, acc_roteq]], dim=-1)\n",
    "    best_inv = (y[:,0] > y[:,1])\n",
    "    best_eq = (y[:,0] < y[:,1])\n",
    "    best_same = (y[:,0] == y[:,1])\n",
    "    \n",
    "    print(f\"I uniquely best: {best_inv.sum() / num_classes}\")\n",
    "    print(f\"E uniquely best: {best_eq.sum() / num_classes}\")\n",
    "    print(f\"I and E equal: {best_same.sum() / num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupied-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ./checkpoints/inat-rot-base31-lp-lr5.0-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-roteq-seed24-lp-lr5.0-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-roteq-base31-lp-lr5.0-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-roteq-seed69-lp-lr5.0-cosine//checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-rotinv-base31-lp-lr5.0-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-rotinv-seed24-lp-lr5.0-cosine/checkpoint_best.pth\n",
      "loaded ./checkpoints/inat-rotinv-seed69-lp-lr5.0-cosine/checkpoint_best.pth\n"
     ]
    }
   ],
   "source": [
    "# INat\n",
    "dataset_name = 'inat-1k'\n",
    "num_classes = 1010\n",
    "lr = '5.0'\n",
    "loader = inat_loader\n",
    "\n",
    "b31 = load_1_model('./checkpoints/inat-rot-base31-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "e24 = load_1_model('./checkpoints/inat-roteq-seed24-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "e31 = load_1_model('./checkpoints/inat-roteq-base31-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "e69 = load_1_model('./checkpoints/inat-roteq-seed69-lp-lr5.0-cosine//checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "i31 = load_1_model('./checkpoints/inat-rotinv-base31-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "i24 = load_1_model('./checkpoints/inat-rotinv-seed24-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "i69 = load_1_model('./checkpoints/inat-rotinv-seed69-lp-lr5.0-cosine/checkpoint_best.pth', full_path=True, num_classes=num_classes)\n",
    "\n",
    "b31_out_inat, b31_tar = rollout_loader(b31, loader)\n",
    "e24_out_inat, e24_tar = rollout_loader(e24, loader)\n",
    "e31_out_inat, e31_tar = rollout_loader(e31, loader)\n",
    "e69_out_inat, e69_tar = rollout_loader(e69, loader)\n",
    "i24_out_inat, i24_tar = rollout_loader(i24, loader)\n",
    "i31_out_inat, i31_tar = rollout_loader(i31, loader)\n",
    "i69_out_inat, i69_tar = rollout_loader(i69, loader)\n",
    "\n",
    "assert(torch.equal(b31_tar, e24_tar))\n",
    "\n",
    "tar_inat = b31_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "owned-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metrics import get_metrics as get_new_metrics\n",
    "\n",
    "def ensem_BEI(all_eq,all_base,all_inv,same_tar,num_E=0, num_B=0, num_I=0, num_comb=5, err='std'):\n",
    "    ee_nll = []\n",
    "    ee_ece = []\n",
    "    ee_acc = []\n",
    "    for i in range(num_comb):\n",
    "        eq_list = np.random.choice(all_eq, num_E, replace=False)\n",
    "        base_list = np.random.choice(all_base, num_B, replace=False)\n",
    "        inv_list = np.random.choice(all_inv, num_I, replace=False)\n",
    "        out_list = list(eq_list) + list(base_list) + list(inv_list)\n",
    "        out_list = [torch.Tensor(x.cpu()) for x in out_list]\n",
    "        ee_out = torch.stack(out_list).softmax(-1).mean(dim=0).cuda()\n",
    "        nll, ece, acc, _ = get_new_metrics([ee_out],[same_tar],[f'EE_comb{i}'], printing=False, input_softmax=True)    \n",
    "        ee_nll.append(nll)\n",
    "        ee_ece.append(ece)  \n",
    "        ee_acc.append(acc)\n",
    "    print(\"E\"*num_E + \"B\"*num_B + \"I\"*num_I)\n",
    "    if err=='std':\n",
    "        print(f\"NLL: {np.mean(ee_nll):.4f} +/- {np.std(ee_nll):.4f}\")\n",
    "        print(f\"ECE: {np.mean(ee_ece):.4f} +/- {np.std(ee_ece):.4f}\")\n",
    "        print(f\"Acc: {np.mean(ee_acc):.4f} +/- {np.std(ee_acc):.4f}\")\n",
    "    elif err=='var':\n",
    "        print(f\"NLL: {np.mean(ee_nll):.4f} +/- {np.var(ee_nll):.4f}\")\n",
    "        print(f\"ECE: {np.mean(ee_ece):.4f} +/- {np.var(ee_ece):.4f}\")\n",
    "        print(f\"Acc: {np.mean(ee_acc):.4f} +/- {np.var(ee_acc):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-silence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "NLL: 1.8833 +/- 0.0060\n",
      "ECE: 0.0450 +/- 0.0024\n",
      "Acc: 0.5504 +/- 0.0019\n",
      "B\n",
      "NLL: 1.8776 +/- 0.0000\n",
      "ECE: 0.0308 +/- 0.0000\n",
      "Acc: 0.5489 +/- 0.0000\n",
      "I\n",
      "NLL: 1.8389 +/- 0.0112\n",
      "ECE: 0.0435 +/- 0.0013\n",
      "Acc: 0.5633 +/- 0.0021\n"
     ]
    }
   ],
   "source": [
    "# 1 models\n",
    "all_eq_f = [e69_out_inat.cpu(), e31_out_inat.cpu(), e24_out_inat.cpu()]\n",
    "all_base_f = [b31_out_inat.cpu(), b31_out_inat.cpu()] #duplicated to prevent err\n",
    "all_inv_f = [i24_out_inat.cpu(), i31_out_inat.cpu(), i69_out_inat.cpu()] #duplicated to prevent err\n",
    "\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=3)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_B=1, num_comb=3)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_I=1, num_comb=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dying-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == \"__main__\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EE\n",
      "NLL: 1.7280 +/- 0.0041\n",
      "ECE: 0.0256 +/- 0.0022\n",
      "Acc: 0.5834 +/- 0.0007\n",
      "EI\n",
      "NLL: 1.6762 +/- 0.0078\n",
      "ECE: 0.0376 +/- 0.0013\n",
      "Acc: 0.5973 +/- 0.0028\n",
      "II\n",
      "NLL: 1.6951 +/- 0.0041\n",
      "ECE: 0.0208 +/- 0.0010\n",
      "Acc: 0.5937 +/- 0.0006\n",
      "EEE\n",
      "NLL: 1.6726 +/- 0.0000\n",
      "ECE: 0.0494 +/- 0.0000\n",
      "Acc: 0.5976 +/- 0.0000\n",
      "EEI\n",
      "NLL: 1.6237 +/- 0.0034\n",
      "ECE: 0.0620 +/- 0.0015\n",
      "Acc: 0.6098 +/- 0.0008\n",
      "EII\n",
      "NLL: 1.6184 +/- 0.0050\n",
      "ECE: 0.0599 +/- 0.0017\n",
      "Acc: 0.6117 +/- 0.0015\n",
      "III\n",
      "NLL: 1.6410 +/- 0.0000\n",
      "ECE: 0.0419 +/- 0.0000\n",
      "Acc: 0.6061 +/- 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 2 models\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=2)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_I=1)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_I=2)\n",
    "\n",
    "# 3 models\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=3)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=2, num_I=1)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_I=2)\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat, num_I=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83f86d-fda7-49b3-ba26-1477c8812c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 models\n",
    "all_eq_f = [e69_out_inat.cpu(), e69_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)\n",
    "\n",
    "all_eq_f = [e31_out_inat.cpu(),e31_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)\n",
    "\n",
    "all_eq_f = [e24_out_inat.cpu(), e24_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)\n",
    "\n",
    "all_eq_f = [i31_out_inat.cpu(), i31_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)\n",
    "\n",
    "all_eq_f = [i24_out_inat.cpu(), i24_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)\n",
    "\n",
    "all_eq_f = [i69_out_inat.cpu(), i69_out_inat.cpu()]\n",
    "ensem_BEI(all_eq_f, all_base_f, all_inv_f,tar_inat,num_E=1, num_comb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "treated-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  if sys.path[0] == \"\":\n",
      "/data/scratch/swhan/anaconda3/envs/pytorch1.12/lib/python3.7/site-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eq\n",
      "NLL: 1.8738282918930054 | ECE: 0.04541622946110569\n",
      "Acc: 0.5531028509140015\n",
      "inv\n",
      "NLL: 1.8309305906295776 | ECE: 0.04260874650000008\n",
      "Acc: 0.5648025274276733\n",
      "I uniquely best: 0.42178216576576233\n",
      "E uniquely best: 0.3316831588745117\n",
      "I and E equal: 0.24653466045856476\n"
     ]
    }
   ],
   "source": [
    "same_tar = b31_tar.cpu()\n",
    "num_comb = 1\n",
    "# all_eq_exR = [eq24_out, eq69_out, eq31_out]\n",
    "# all_base_exR = [b24_out, b24_out, b31_out]\n",
    "# all_inv_exR = [inv24_out, inv69_out, inv31_out]\n",
    "all_eq_exR = all_eq_f\n",
    "all_base_exR = all_base_f\n",
    "all_inv_exR = all_inv_f\n",
    "for i in range(num_comb):\n",
    "    [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "    # [b1] = np.random.choice(all_base_exR, 1)\n",
    "    [inv1] = np.random.choice(all_inv_exR, 1) \n",
    "    # _,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "    get_classwise_ei(acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100\n",
    "dataset_name = 'cifar100'\n",
    "num_classes = 100\n",
    "lr = '0.2'\n",
    "loader = c100_loader\n",
    "\n",
    "b69 = load_1_model(f\"trans_{dataset_name}_base69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b69_out, b69_tar = rollout_loader(b69, loader)\n",
    "same_tar = b69_tar\n",
    "get_metrics([b69_out], [same_tar],['b69'], num_classes=num_classes)\n",
    "eq69 = load_1_model(f\"trans_{dataset_name}_eq69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq69_out, eq69_tar = rollout_loader(eq69, loader)\n",
    "get_metrics([eq69_out], [same_tar],['eq69'], num_classes=num_classes)\n",
    "\n",
    "inv69 = load_1_model(f\"trans_{dataset_name}_inv69_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv69_out, inv69_tar = rollout_loader(inv69, loader)\n",
    "get_metrics([inv69_out], [same_tar],['inv69'], num_classes=num_classes)\n",
    "\n",
    "assert(torch.equal(b69_tar,eq69_tar))\n",
    "\n",
    "b24 = load_1_model(f\"trans_{dataset_name}_base24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b24_out, _ = rollout_loader(b24, loader)\n",
    "get_metrics([b24_out], [same_tar],['b24'], num_classes=num_classes)\n",
    "\n",
    "eq24 = load_1_model(f\"trans_{dataset_name}_eq24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq24_out, _ = rollout_loader(eq24, loader)\n",
    "get_metrics([eq24_out], [same_tar],['eq24'], num_classes=num_classes)\n",
    "\n",
    "inv24 = load_1_model(f\"trans_{dataset_name}_inv24_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv24_out, _ = rollout_loader(inv24, loader)\n",
    "get_metrics([inv24_out], [same_tar],['inv24'], num_classes=num_classes)\n",
    "\n",
    "\n",
    "b31 = load_1_model(f\"trans_{dataset_name}_base31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "b31_out, _ = rollout_loader(b31, loader)\n",
    "get_metrics([b31_out], [same_tar],['b31'], num_classes=num_classes)\n",
    "\n",
    "eq31 = load_1_model(f\"trans_{dataset_name}_eq31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq31_out, _ = rollout_loader(eq31, loader)\n",
    "get_metrics([eq31_out], [same_tar],['eq31'], num_classes=num_classes)\n",
    "\n",
    "inv31 = load_1_model(f\"trans_{dataset_name}_inv31_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "inv31_out, _ = rollout_loader(inv31, loader)\n",
    "get_metrics([inv31_out], [same_tar],['inv31'], num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar100'\n",
    "num_classes = 100\n",
    "lr = '0.2'\n",
    "loader = c100_loader\n",
    "\n",
    "eq42 = load_1_model(f\"trans_{dataset_name}_eq42_cos_lr{lr}_bs256\", num_classes=num_classes)\n",
    "eq42_out, eq42_tar = rollout_loader(eq42, loader)\n",
    "get_metrics([eq42_out], [eq42_tar],['eq42'], num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tar = eq42_tar\n",
    "all_eq = [eq69_out, eq24_out, eq31_out, eq42_out]\n",
    "all_base = [b69_out, b24_out, b31_out]\n",
    "all_inv = [inv69_out, inv24_out, inv31_out]\n",
    "\n",
    "# 1 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar, num_E=1)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_B=1)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_I=1)\n",
    "\n",
    "# 2 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=2)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=1, num_I=1)\n",
    "\n",
    "# 3 models\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=3)\n",
    "ensem_BEI(all_eq, all_base, all_inv, same_tar,num_E=2, num_I=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "(91.2+91.9)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "(91.2+91.9+91.9)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "87.03-(85.5+84.0+85.5)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "86.54-(85.5+84.0)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = b24_out\n",
    "eq1 = eq42_out\n",
    "inv1 = inv69_out\n",
    "\n",
    "_,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "_,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "_,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "get_classwise(acc_pc_b, acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tar = b69_tar\n",
    "num_comb = 1\n",
    "all_eq_exR = [eq24_out, eq69_out, eq31_out]\n",
    "all_base_exR = [b24_out, b69_out, b31_out]\n",
    "all_inv_exR = [inv24_out, inv69_out, inv31_out]\n",
    "for i in range(num_comb):\n",
    "    [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "    [b1] = np.random.choice(all_base_exR, 1)\n",
    "    [inv1] = np.random.choice(all_inv_exR, 1) \n",
    "    _,_,_,acc_pc_b = get_metrics([b1], [same_tar],['base'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_eq = get_metrics([eq1], [same_tar],['eq'], num_classes=num_classes)\n",
    "    _,_,_,acc_pc_inv = get_metrics([inv1], [same_tar],['inv'], num_classes=num_classes)\n",
    "    get_classwise(acc_pc_b, acc_pc_inv, acc_pc_eq, num_classes=num_classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comb =1 \n",
    "eee_nll = []\n",
    "eee_ece = []\n",
    "eee_acc = []\n",
    "for i in range(num_comb):\n",
    "    [eq1, eq2, eq3] = np.random.choice(all_eq_exR, 3, replace=False)\n",
    "    eee_out = (eq1.softmax(-1) + eq2.softmax(-1) + eq3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([eee_out],[same_tar],[f'EEE_comb{i}'], input_softmax=True, num_classes=num_classes)    \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(eq1,eq2,eq3, same_tar)\n",
    "\n",
    "    eee_nll.append(nll)\n",
    "    eee_ece.append(ece)  \n",
    "    eee_acc.append(acc)   \n",
    "\n",
    "bbb_nll = []\n",
    "bbb_ece = []\n",
    "bbb_acc = []\n",
    "for i in range(num_comb):\n",
    "    [b1, b2, b3] = np.random.choice(all_base_exR, 3, replace=False)\n",
    "    bbb_out = (b1.softmax(-1) + b2.softmax(-1) + b3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([bbb_out],[same_tar],[f'BBB_comb{i}'], input_softmax=True, num_classes=num_classes)      \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(b1,b2,b3, same_tar)\n",
    "\n",
    "    bbb_nll.append(nll)\n",
    "    bbb_ece.append(ece)  \n",
    "    bbb_acc.append(acc) \n",
    "\n",
    "iii_nll = []\n",
    "iii_ece = []\n",
    "iii_acc = []\n",
    "for i in range(num_comb):\n",
    "    [i1, i2, i3] = np.random.choice(all_inv_exR, 3, replace=False)\n",
    "    iii_out = (i1.softmax(-1) + i2.softmax(-1) + i3.softmax(-1))/3\n",
    "    nll, ece, acc, _ = get_metrics([iii_out],[same_tar],[f'III_comb{i}'], input_softmax=True, num_classes=num_classes)      \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(i1,i2,i3, same_tar)\n",
    "    \n",
    "    iii_nll.append(nll)\n",
    "    iii_ece.append(ece)  \n",
    "    iii_acc.append(acc) \n",
    "    \n",
    "bei_nll = []\n",
    "bei_ece = []\n",
    "bei_acc = []\n",
    "for i in range(num_comb):\n",
    "#     [eq1] = np.random.choice(all_eq_exR, 1)\n",
    "#     [base1] = np.random.choice(all_base_exR, 1)\n",
    "#     [inv1] = np.random.choice(all_inv_exR, 1)\n",
    "    [eq1] = np.random.choice([eq31_out], 1)\n",
    "    [base1] = np.random.choice([b31_out], 1)\n",
    "    [inv1] = np.random.choice([inv24_out], 1)\n",
    "    bei_out = (eq1.softmax(-1) + base1.softmax(-1) + inv1.softmax(-1))/3\n",
    "    \n",
    "    nll, ece, acc, _ = get_metrics([bei_out],[same_tar],[f'BEI_comb{i}'], input_softmax=True, num_classes=num_classes)     \n",
    "    ag, dag, kld, std_logits, std = get_div_metrics(eq1,base1,inv1, same_tar)\n",
    "    \n",
    "    bei_nll.append(nll)\n",
    "    bei_ece.append(ece)  \n",
    "    bei_acc.append(acc) \n",
    "\n",
    "print(f\"\\nEEE Acc: {np.mean(eee_acc)} +/- {np.std(eee_acc)}\")\n",
    "print(f\"EEE ECE: {np.mean(eee_ece)} +/- {np.std(eee_ece)}\")\n",
    "print(f\"EEE NLL: {np.mean(eee_nll)} +/- {np.std(eee_nll)}\")\n",
    "\n",
    "print(f\"\\nBBB Acc: {np.mean(bbb_acc)} +/- {np.std(bbb_acc)}\")\n",
    "print(f\"BBB ECE: {np.mean(bbb_ece)} +/- {np.std(bbb_ece)}\")\n",
    "print(f\"BBB NLL: {np.mean(bbb_nll)} +/- {np.std(bbb_nll)}\")\n",
    "\n",
    "print(f\"\\nIII Acc: {np.mean(iii_acc)} +/- {np.std(iii_acc)}\")\n",
    "print(f\"III ECE: {np.mean(iii_ece)} +/- {np.std(iii_ece)}\")\n",
    "print(f\"III NLL: {np.mean(iii_nll)} +/- {np.std(iii_nll)}\")\n",
    "\n",
    "print(f\"\\nBEI Acc: {np.mean(bei_acc)} +/- {np.std(bei_acc)}\")\n",
    "print(f\"BEI ECE: {np.mean(bei_ece)} +/- {np.std(bei_ece)}\")\n",
    "print(f\"BEI NLL: {np.mean(bei_nll)} +/- {np.std(bei_nll)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(b24_out,eq24_out,inv24_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(eq69_out,eq24_out,eq31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(inv69_out,inv24_out,inv31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, dag, kld, std_logits, std = get_div_metrics(b69_out,b24_out,b31_out, same_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-tunisia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
